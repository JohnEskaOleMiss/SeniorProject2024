{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmYtTVYV49GJ",
        "outputId": "047c7194-b0c2-4baa-8abf-06655cf2516b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import os\n",
        "from time import sleep\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#preprocess Phishing_Email.csv for Naive Bayes, Decision Tree, SVC\n",
        "def cleanDataNaiveDecisionSVC(data):\n",
        "  #remove all duplicates and empty lines\n",
        "  data = data.dropna()\n",
        "  data = data.drop_duplicates()\n",
        "  data = data[data['Email Text'] != 'empty']\n",
        "\n",
        "  #remove special characters, html tags, \\n, urls, and extra spaces\n",
        "  #data['Email Text'] = data['Email Text'].apply(lambda x: re.sub(r'[^\\w\\s]', \" \", x))\n",
        "  data['Email Text'] = data['Email Text'].apply(lambda x: re.sub('http\\S+|www\\S+|https\\S+',\" \", x, flags=re.MULTILINE))\n",
        "  data['Email Text'] = data['Email Text'].apply(lambda x: re.sub('\\S{15,}',\" \", x))\n",
        "  data['Email Text'] = data['Email Text'].apply(lambda x: re.sub('[^a-zA-Z\\s]',\" \", x))\n",
        "  data['Email Text'] = data['Email Text'].apply(lambda x: re.sub('<.*?>',\" \", x))\n",
        "  data['Email Text'] = data['Email Text'].apply(lambda x: re.sub('\\\\\\n',\" \", x))\n",
        "  data['Email Text'] = data['Email Text'].apply(lambda x: re.sub('\\s+', ' ', x).strip())\n",
        "  data['Email Text'] = data['Email Text'].apply(lambda x: x.strip())\n",
        "  data['Email Text'] = data['Email Text'].str.lower()\n",
        "\n",
        "  #remove all stopwords\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  data['Email Text'] = data['Email Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
        "\n",
        "  #turn the new data into a csv to make sure there are no new empty lines after removing everything above\n",
        "  data.to_csv('data.csv', index=False)\n",
        "  sleep(2)\n",
        "  data = pd.read_csv(\"data.csv\")\n",
        "  data = data.dropna()\n",
        "\n",
        "  #split data into safe and phishing groups and get length\n",
        "  safe_df = data[data[\"Email Type\"] == 'Safe Email']\n",
        "  phishing_df = data[data[\"Email Type\"] == 'Phishing Email']\n",
        "  lensd = len(safe_df)\n",
        "  lenpd = len(phishing_df)\n",
        "\n",
        "  #randomize data\n",
        "  # safe_df = safe_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "  # phishing_df = phishing_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "  #even out the size of each group\n",
        "  if len(safe_df) > len(phishing_df):\n",
        "      num = lensd-lenpd\n",
        "      rows_to_drop = safe_df.sample(n=num, random_state=42).index\n",
        "      safe_df = safe_df.drop(rows_to_drop)\n",
        "  if len(safe_df) < len(phishing_df):\n",
        "      num = lenpd-lensd\n",
        "      rows_to_drop = phishing_df.sample(n=num, random_state=42).index\n",
        "      phishing_df = phishing_df.drop(rows_to_drop)\n",
        "\n",
        "  #combine the groups back together into 2 seperate groups: training and testing\n",
        "  lensd = len(safe_df)\n",
        "  lenpd = len(phishing_df)\n",
        "\n",
        "  safe_midpoint = len(safe_df) // 2\n",
        "  phishing_midpoint = len(phishing_df) // 2\n",
        "\n",
        "  safe_training = safe_df.iloc[:safe_midpoint]\n",
        "  safe_test = safe_df.iloc[safe_midpoint:]\n",
        "\n",
        "  phishing_training = phishing_df.iloc[:phishing_midpoint]\n",
        "  phishing_test = phishing_df.iloc[phishing_midpoint:]\n",
        "\n",
        "  training_df = pd.concat([safe_training, phishing_training])\n",
        "  test_df = pd.concat([safe_test, phishing_test])\n",
        "\n",
        "  #create csv and return array of training and test dataset\n",
        "  training_df_shuffle = training_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "  training_df_shuffle.to_csv('train_data_NDS_shuffle.csv', index=False)\n",
        "  training_df.to_csv('train_data_NDS.csv', index=False)\n",
        "  test_df.to_csv('test_data_NDS.csv', index=False)\n",
        "  #data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "  d = [training_df, test_df]\n",
        "\n",
        "  return d\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import os\n",
        "from time import sleep\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#preprocess Phishing_Email.csv for BERT\n",
        "def cleanDataBert(data):\n",
        "  data = data.dropna()\n",
        "  data = data.drop_duplicates()\n",
        "  data = data[data['Email Text'] != 'empty']\n",
        "\n",
        "  #get 3486 of each type to match size of test set\n",
        "  safe_df = data[data[\"Email Type\"] == 'Safe Email']\n",
        "  phishing_df = data[data[\"Email Type\"] == 'Phishing Email']\n",
        "\n",
        "  safe_df = safe_df.sample(n=6972, random_state=42)\n",
        "  phishing_df = phishing_df.sample(n=6972, random_state=42)\n",
        "\n",
        "  len_safe = len(safe_df)\n",
        "  len_phish = len(phishing_df)\n",
        "\n",
        "  safe_midpoint = len_safe // 2\n",
        "  phishing_midpoint = len_phish // 2\n",
        "\n",
        "  safe_train_df = safe_df.iloc[:safe_midpoint]\n",
        "  safe_test_df = safe_df.iloc[safe_midpoint:]\n",
        "\n",
        "  phishing_train_df = phishing_df.iloc[:phishing_midpoint]\n",
        "  phishing_test_df = phishing_df.iloc[phishing_midpoint:]\n",
        "\n",
        "  #combine the groups together\n",
        "  train_df = pd.concat([safe_train_df, phishing_train_df])\n",
        "  test_df = pd.concat([safe_test_df, phishing_test_df])\n",
        "\n",
        "  #reset the index\n",
        "  train_df = train_df.reset_index(drop=True)\n",
        "  test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "  #shuffle both\n",
        "  train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
        "  test_df = test_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "  #turn to csv\n",
        "  train_df.to_csv(\"train_data_BERT.csv\", index=False)\n",
        "  test_df.to_csv(\"test_data_BERT.csv\", index=False)\n",
        "\n",
        "\n",
        "cleanDataBert(pd.read_csv(\"Phishing_Email.csv\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCAKa8Tg5oFj",
        "outputId": "1d23f374-0c5d-460f-d7d3-04741ed66b44"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import os\n",
        "from time import sleep\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#preprocess Phishing_Email.csv for OpenAI\n",
        "def cleanDataOpenAI(data):\n",
        "  data = data.dropna()\n",
        "  data = data.drop_duplicates()\n",
        "  data = data[data['Email Text'] != 'empty']\n",
        "\n",
        "  #get 3486 of each type to match size of test set\n",
        "  safe_df = data[data[\"Email Type\"] == 'Safe Email']\n",
        "  phishing_df = data[data[\"Email Type\"] == 'Phishing Email']\n",
        "\n",
        "  safe_df = safe_df.sample(n=3486, random_state=42)\n",
        "  phishing_df = phishing_df.sample(n=3486, random_state=42)\n",
        "\n",
        "  #combine the groups together\n",
        "  test_df = pd.concat([safe_df, phishing_df])\n",
        "\n",
        "  #create csv\n",
        "  # test_df.to_csv('test_data_openai.csv', index=False)\n",
        "\n",
        "  #shuffle for another csv\n",
        "  test_df_shuffle = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "  test_df_shuffle.to_csv('test_data_openai.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRa8mBwW5Yuz",
        "outputId": "d9513118-fd64-4589-94c3-bec39b5e06eb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def main():\n",
        "  df = pd.read_csv(\"Phishing_Email.csv\")\n",
        "  cleanDataNaiveDecisionSVC(df)\n",
        "  cleanDataBert(df)\n",
        "  cleanDataOpenAI(df)\n",
        "  print(\"Done\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQdZCIEz6Dwt",
        "outputId": "898f6375-82b1-41c8-c2bf-e5362d356b24"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    }
  ]
}